---
title: "Woodman et al (in review) - Example analysis"
author: "Samuel Woodman"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Woodman et al (in review) - Example analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
```

```{r setup, message=FALSE}
library(dplyr)
library(eSDM)
library(lwgeom)
library(maps)
library(maptools)
library(sf)
library(tmap)

source(system.file("vignette_helper.R", package = "eSDM"), local = TRUE, echo = FALSE)
```

#### If using data from this example analysis, please first contact the author, and then see the README.txt file in 'inst/extdata' for proper citation information.

`eSDM` allows users to create ensembles of predictions from species distribution models (SDMs). They can do this either using the `eSDM` GUI or manually using `eSDM` functions. This vignette demonstrates creating and evaluating ensembles using `eSDM` functions by manually performing the example analysis from Woodman _et al._ (in review). The example analysis explores differences between blue whale SDM predictions from Becker et al. (2016; i.e. Model_B), Hazen et al. (2017; i.e., Model_H), and Redfern et al. (2017; i.e., Model_R), and creates ensembles of the predictions, with associated uncertainty. See Table 1 of Woodman _et al._ (in review) for additional details about the differences between the models.

This document contains code for plotting predictions, but by default the plotting code is not run because these plots take a while to generate.


## Import SDM predictions

The first step of the example analysis is to import and process the Model_B, Model_H, and Model_R predictions, along with their standard error (SE) values. Use `pts2poly_centroids` to create `sf` objects from polygon centroids from .csv files and `sf::st_read` to import a shapefile. The dimensions of the Model_B and Model_H predictions are 0.09 x 0.09 degrees and 0.25 x 0.25 degrees, respectively; the second argument of `pts2poly_centroids` is half the length of one side of the polygon. GIS files already have a defined geometry that is read by `st_read`. We must ensure that the predictions have the same longitudinal range (i.e., [0, 360] or [-180, 180]), and that all geometries are valid.

After importing and processing the predictions, we can visualize them using the custom `plot_sf_3panel` function (in 'inst/vignette_helper.R'), as well as a base map for reference. These plots are in Fig. 3 in Woodman _et al._ (in review).

```{r}
# Import, process, and plot predictions from Model_B
eab.sf <- read.csv(system.file("extdata/Predictions_Beckeretal2016.csv", package = "eSDM")) %>% 
  eSDM::pts2poly_centroids(0.09 / 2, crs = 4326) %>%
  st_wrap_dateline() %>%
  st_set_agr("constant")

eab.sf

# Make base map
map.world <- st_geometry(st_as_sf(map('world', plot = FALSE, fill = TRUE)))
```

```{r, fig.width=7, eval=FALSE}
plot_sf_3panel(eab.sf, "pred_bm", main.txt = "Model_B - ", map.base = map.world)
```

```{r}
# Import, process, and plot predictions from Model_H
ww.sf <- read.csv(system.file("extdata/Predictions_Hazenetal2017.csv", package = "eSDM")) %>%
  select(lon, lat, pred_bm, se) %>%
  eSDM::pts2poly_centroids(0.25 / 2, crs = 4326, agr = "constant")

ww.sf
```

```{r, fig.width=7, eval=FALSE}
plot_sf_3panel(ww.sf, "pred_bm", main.txt = "Model_H - ", map.base = map.world)
```

```{r, fig.width=7}
# Import, process, and plot predictions from Model_R
jvr.sf <- st_read(system.file("extdata/Shapefiles/Predictions_Redfernetal2017.shp", package = "eSDM")) %>%
  st_make_valid() %>% #
  st_set_agr("constant")

jvr.sf
```

```{r, fig.width=7, eval=FALSE}
plot_sf_3panel(jvr.sf, "pred_bm", main.txt = "Model_R - ", map.base = map.world)
```


## Overlay predictions

Because the original predictions have both different spatial resolutions and coordinate systems, we must overlay them onto the same base geometry. See Woodman _et al._ (in review), the `eSDM` GUI manual, or the `overlay_sdm` documentation for details about the overlay process. For the example analysis, we use the geometry of Model_R as the base geometry. However, first we must import and process the study area and erasing polygons (with which we clip and erase land from the base geometry, respectively). Then we can visualize the base geometry, as well as a base map for reference.

```{r}
# Study area polygon
poly.study <- st_read(system.file("extdata/Shapefiles/Study_Area_CCE.shp", package = "eSDM")) %>%
  st_geometry() %>% 
  st_transform(st_crs(jvr.sf))

# Erasing polygon; clip to the buffered study area polygon to reduce future computation time
poly.erase <- eSDM::gshhg.l.L16 %>%
  st_transform(st_crs(jvr.sf)) %>%
  lwgeom::st_make_valid() %>%
  st_crop(st_buffer(poly.study, 100000))

# Create the base geometry
base.geom <- jvr.sf %>%
  st_geometry() %>%
  st_erase(poly.erase) %>%
  st_intersection(poly.study) %>%
  st_cast("MULTIPOLYGON")
```

```{r, fig.width=5, fig.height=7, eval=FALSE}
# visualize the base geometry
plot(st_transform(base.geom, 4326), col = NA, border = "black", axes = TRUE)
plot(map.world, add = TRUE, col = "tan", border = NA)
graphics::box()
```

Next, we must convert any associated uncertainty values from to variance; uncertainty values must be overlaid as variance values to remain statistically valid (REF). 

```{r}
# Convert SE values to variance
eab.sf <- eab.sf %>% 
  mutate(variance = se^2) %>% 
  select(pred_bm, se, variance)
ww.sf <- ww.sf %>% 
  mutate(variance = se^2) %>% 
  select(pred_bm, se, variance)
jvr.sf <- jvr.sf %>% 
  mutate(variance = se^2) %>% 
  select(pred_bm, se, variance)
```

Now we can overlay the original predictions onto the base geometry. Note that because we clipped and erased land from the base geometry, we cannot simply use the original Model_R predictions and geometry. However, we can 'overlay' the Model_R predictions by matching indices of base geometry polygons and Model_R prediction values (`over3.sfb` in the code below).

```{r}
# Perform overlay, and convert uncertainty values back to SEs
over1.sf <- eSDM::overlay_sdm(base.geom, st_transform(eab.sf, st_crs(base.geom)), c("pred_bm", "variance"), 50) %>% 
  mutate(se = sqrt(variance))
over2.sf <- eSDM::overlay_sdm(base.geom, st_transform(ww.sf, st_crs(base.geom)), c("pred_bm", "variance"), 50) %>% 
  mutate(se = sqrt(variance))
over3.sf <- eSDM::overlay_sdm(base.geom, jvr.sf, c("pred_bm", "variance"), 50) %>% 
  mutate(se = sqrt(variance))

over3.sfb <- jvr.sf %>% 
  st_set_geometry(NULL) %>% 
  select(pred_bm, variance) %>% 
  st_sf(geometry = base.geom, agr = "constant") %>% 
  dplyr::mutate(se = sqrt(variance))
all.equal(over3.sf, over3.sfb)
rm(over3.sfb)

```

We can plot the overlaid predictions if desired to show that the overlaid distribution patterns are very similar to those of the original predictions.

```{r, fig.width=7, eval=FALSE}
plot_sf_3panel(over1.sf, "pred_bm", main.txt = "Overlaid Model_B - ", map.base = map.world)
plot_sf_3panel(over2.sf, "pred_bm", main.txt = "Overlaid Model_H - ", map.base = map.world)
plot_sf_3panel(over3.sf, "pred_bm", main.txt = "Overlaid Model_R - ", map.base = map.world)
```


## Calculate evaluation metrics

To evaluate predictions and create ensembles with weights based on evaluation metrics, we must load and process the validation data sets.  We use three validation sets in the example analysis, line transect data (Becker _et al._ 2016), home range data (Irvine _et al._ 2014), and these two data sets combined. Because our validation data is binary (i.e., presence/absence), it must have a column indicating whether it is a presence point (value of 1) or absence point (value of 0).

```{r}
# Import and process validation data
valid.data <- read.csv(system.file("extdata/eSDM_Validation_data_all.csv", package = "eSDM"), 
                       stringsAsFactors = FALSE) %>% 
  arrange(source, lat, lon) %>% 
  mutate(pres_abs = ifelse(pres_abs > 0, 1, 0)) %>% #For demonstration purposes; pres_abs column is already binary
  st_as_sf(coords = c("lon", "lat"), crs = 4326, agr = "constant") %>%
  st_transform(st_crs(base.geom))

# Extract the line transect and home range validation data
valid.data.lt <- valid.data %>% filter(source == "Becker_et_al_2016")
valid.data.hr <- valid.data %>% filter(source == "Irvine_et_al_2014")

# Summarize the number of presence and absence points
valid.data %>% 
  st_set_geometry(NULL) %>% 
  group_by(source) %>%  
  summarize(pres = sum(pres_abs == 1), 
            abs = sum(pres_abs == 0)) %>% 
  knitr::kable(caption = "Validation data summary")
```

Now we can calculate the AUC and TSS metrics for the original and overlaid predictions. However, note the following code chunk is not run; the displayed table is Table 3 from Woodman _et al._ (in review).

```{r, eval=FALSE}
names.1 <- c(
  "Model_B_orig", "Model_H_orig", "Model_R_orig", 
  "Model_B_overlaid", "Model_H_overlaid", "Model_R_overlaid"
)

eval.lt <- data.frame(do.call(rbind, list(
  eSDM::evaluation_metrics(eab.sf, 1, st_transform(valid.data.lt, 4326), "pres_abs"), 
  eSDM::evaluation_metrics(ww.sf, 1, st_transform(valid.data.lt, 4326), "pres_abs"), 
  eSDM::evaluation_metrics(jvr.sf, 1, valid.data.lt, "pres_abs"), 
  eSDM::evaluation_metrics(over1.sf, 1, valid.data.lt, "pres_abs"), 
  eSDM::evaluation_metrics(over2.sf, 1, valid.data.lt, "pres_abs"), 
  eSDM::evaluation_metrics(over3.sf, 1, valid.data.lt, "pres_abs")
))) %>%
  mutate(Preds = names.1) %>%
  dplyr::select(Preds, AUC_LT = X1, TSS_LT = X2)

eval.hr <- data.frame(do.call(rbind, list(
  eSDM::evaluation_metrics(eab.sf, 1, st_transform(valid.data.hr, 4326), "pres_abs"), 
  eSDM::evaluation_metrics(ww.sf, 1, st_transform(valid.data.hr, 4326), "pres_abs"), 
  eSDM::evaluation_metrics(jvr.sf, 1, valid.data.hr, "pres_abs"), 
  eSDM::evaluation_metrics(over1.sf, 1, valid.data.hr, "pres_abs"), 
  eSDM::evaluation_metrics(over2.sf, 1, valid.data.hr, "pres_abs"), 
  eSDM::evaluation_metrics(over3.sf, 1, valid.data.hr, "pres_abs")
))) %>%
  mutate(Preds = names.1) %>%
  dplyr::select(Preds, AUC_HR = X1, TSS_HR = X2)

eval.combo <- data.frame(do.call(rbind, list(
  eSDM::evaluation_metrics(eab.sf, 1, st_transform(valid.data, 4326), "pres_abs"), 
  eSDM::evaluation_metrics(ww.sf, 1, st_transform(valid.data, 4326), "pres_abs"), 
  eSDM::evaluation_metrics(jvr.sf, 1, valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(over1.sf, 1, valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(over2.sf, 1, valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(over3.sf, 1, valid.data, "pres_abs")
))) %>%
  mutate(Preds = names.1) %>%
  dplyr::select(Preds, AUC = X1, TSS = X2)
```

```{r}
read.csv(system.file("extdata/Table3.csv", package = "eSDM")) %>%
  filter(grepl("Model_", Predictions)) %>% 
  select(Predictions, AUC, TSS, `AUC-LT` = AUC.LT, `TSS-LT` = TSS.LT, 
         `AUC-HR` = AUC.HR, `TSS-HR` = TSS.HR) %>% 
  knitr::kable(caption = "Evaluation metrics", digits = 3, align = "lcccccc")
```


We can see that for each set of predictions, the original and overlaid evaluation metrics are quite similar. This confirms that the overlay conserved the predicted distributions.


## Create ensemble predictions

Before creating the ensembles, we must rescale the overlaid predictions. We will rescale the predictions using the abundance rescaling method so they all predict an abundance of 1648 (Becker _et al._ 2016). Using the sum to 1 rescaling method would result in ensembles with similar distribution patterns, but the actual density values could not be used to provide a meaningful abundance estimate.

```{r}
# Rescale predictions
over.sf <- bind_cols(
  st_set_geometry(over1.sf, NULL) %>% select(pred_bm1 = pred_bm, var1 = variance), 
  st_set_geometry(over2.sf, NULL) %>% select(pred_bm2 = pred_bm, var2 = variance), 
  st_set_geometry(over3.sf, NULL) %>% select(pred_bm3 = pred_bm, var3 = variance)
) %>% 
  st_sf(geometry = base.geom, agr = "constant")

over.sf.rescaled <- ensemble_rescale(
  over.sf, c("pred_bm1", "pred_bm2", "pred_bm3"), "abundance", 1648, 
  x.var.idx = c("var1", "var2", "var3")
)

# Check that overlaid predictions predict expected abundance
eSDM::model_abundance(over.sf.rescaled, "pred_bm1")
eSDM::model_abundance(over.sf.rescaled, "pred_bm2")
eSDM::model_abundance(over.sf.rescaled, "pred_bm3")

summary(over.sf.rescaled)
```

We also must calculate the ensemble weights. For the example analysis, we create ensembles using several weighting methods: equal weights (i.e., unweighted), AUC-based weights, TSS-based weights, and weights calculated as the inverse of the prediction variance. Each set of weights should sum to 1, or in the case of the weights calculated as the inverse of the prediction variance, each row should sum to 1.

```{r}
# Calculate ensemble weights
e.weights <- list(
  eSDM::evaluation_metrics(over1.sf, 1, valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(over2.sf, 1, valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(over3.sf, 1, valid.data, "pres_abs")
)

over.df.resc.var <- over.sf.rescaled %>% 
  select(var1, var2, var3) %>% 
  st_set_geometry(NULL)

e.weights.unw <- c(1, 1, 1) / 3
e.weights.auc <- sapply(e.weights, function(i) i[1]) / sum(sapply(e.weights, function(i) i[1]))
e.weights.tss <- sapply(e.weights, function(i) i[2]) / sum(sapply(e.weights, function(i) i[2]))
e.weights.var <- data.frame(t(apply(
  1 / over.df.resc.var, 1, function(i) {i / sum(i, na.rm = TRUE)}
)))

e.weights.unw
e.weights.auc
e.weights.tss
head(e.weights.var)
```

Finally, we can create the ensembles. We calculate the ensemble uncertainty values with the among-model variance.

```{r}
### Create ensembles

# Unweighted; calculate CV because it is used in Fig. 4 plot
ens.sf.unw <- eSDM::ensemble_create(
  over.sf.rescaled, c("pred_bm1", "pred_bm2", "pred_bm3"),  w = e.weights.unw, 
  x.var.idx = NULL
) %>% 
  mutate(SE = sqrt(Var_ens), CV = SE / Pred_ens) %>% 
  select(Pred_ens, SE, CV) %>% 
  st_set_agr("constant")

# Weights based on AUC
ens.sf.wauc <- eSDM::ensemble_create(
  over.sf.rescaled, c("pred_bm1", "pred_bm2", "pred_bm3"),  w = e.weights.auc, 
  x.var.idx = NULL
) %>% 
  mutate(SE = sqrt(Var_ens)) %>% 
  select(Pred_ens, SE) %>% 
  st_set_agr("constant")

# Weights based on TSS
ens.sf.wtss <- eSDM::ensemble_create(
  over.sf.rescaled, c("pred_bm1", "pred_bm2", "pred_bm3"),  w = e.weights.tss, 
  x.var.idx = NULL
) %>% 
  mutate(SE = sqrt(Var_ens)) %>% 
  select(Pred_ens, SE) %>% 
  st_set_agr("constant")

# Weights based on the inverse of the variance
ens.sf.wvar <- eSDM::ensemble_create(
  over.sf.rescaled, c("pred_bm1", "pred_bm2", "pred_bm3"),  w = e.weights.var, 
  x.var.idx = NULL
) %>% 
  mutate(SE = sqrt(Var_ens)) %>% 
  select(Pred_ens, SE) %>% 
  st_set_agr("constant")
```

You can also estimate ensemble uncertainty using the within-model uncertainty by specifying arguments for `x.var.idx`, as demonstrated in the following code (not run).

```{r, eval=FALSE}
# Create an ensemble and calculate within-model uncertainty:
ens.sf.unw.wmv <- eSDM::ensemble_create(
  over.sf.rescaled, c("pred_bm1", "pred_bm2", "pred_bm3"),  w = e.weights.unw,
  x.var.idx = c(var1, var2, var3)
) %>%
  mutate(SE = sqrt(Var_ens)) %>%
  select(Pred_ens , SE)
```

Now we can calculate AUC and TSS scores for each and then compare evaluation metrics of original and ensemble predictions. Again, note that the following code chunk is not run; the displayed table is Table 3 from Woodman _et al._ (in review).

```{r, eval=FALSE}
# Calculate evaluation metrics for ensembles
names.2 <- c(
  "Ensemble – unweighted", "Ensemble – AUC-based weights",
  "Ensemble – TSS-based weights", "Ensemble – variance-based weights"
)

eval.lt.ens <- data.frame(do.call(rbind, list(
  eSDM::evaluation_metrics(ens.sf.unw,  "Pred_ens", valid.data.lt, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wauc, "Pred_ens", valid.data.lt, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wtss, "Pred_ens", valid.data.lt, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wvar, "Pred_ens", valid.data.lt, "pres_abs")
))) %>%
  mutate(Preds = names.2) %>%
  dplyr::select(Preds, AUC_LT = X1, TSS_LT = X2)

eval.hr.ens <- data.frame(do.call(rbind, list(
  eSDM::evaluation_metrics(ens.sf.unw,  "Pred_ens", valid.data.hr, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wauc, "Pred_ens", valid.data.hr, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wtss, "Pred_ens", valid.data.hr, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wvar, "Pred_ens", valid.data.hr, "pres_abs")
))) %>%
  mutate(Preds = names.2) %>%
  dplyr::select(Preds, AUC_HR = X1, TSS_HR = X2) 

eval.combo.ens <- data.frame(do.call(rbind, list(
  eSDM::evaluation_metrics(ens.sf.unw,  "Pred_ens", valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wauc, "Pred_ens", valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wtss, "Pred_ens", valid.data, "pres_abs"), 
  eSDM::evaluation_metrics(ens.sf.wvar, "Pred_ens", valid.data, "pres_abs")
))) %>%
  mutate(Preds = names.2) %>%
  dplyr::select(Preds, AUC = X1, TSS = X2)
```

```{r}
read.csv(system.file("extdata/Table3.csv", package = "eSDM")) %>%
  select(Predictions, AUC, TSS, `AUC-LT` = AUC.LT, `TSS-LT` = TSS.LT, 
         `AUC-HR` = AUC.HR, `TSS-HR` = TSS.HR) %>% 
  knitr::kable(caption = "Evaluation metrics", digits = 3, align = "lcccccc")
```

We can see that the ensemble with weights based on TSS values had the highest scores of the ensemble predictions, and mostly higher scores that the original predictions. Below is code to visualize this ensemble (Fig. 5 in Woodman _et al._) and the unweighted ensemble (Fig. 4 in Woodman _et al._) using custom functions (in 'inst/vignette_helper.R') that leverage the `tmap` package to generate plots. However, by default this code is not run because these plots take several minutes to generate.

```{r, fig.width=7, fig.height=4, eval=FALSE}
### Figure 4

# Values passed to tmap.sdm - range of map
range.poly <- st_sfc(
  st_polygon(list(matrix(
    c(-132, -132, -116, -116, -132, 29.5, 49, 49, 29.5, 29.5), ncol = 2
  ))),
  crs = 4326
)
rpoly.mat <- matrix(st_bbox(range.poly), ncol = 2)

# Values passed to tmap.sdm - size of text labels and legend width
main.size <- 0.8
leg.size  <- 0.6
leg.width <- 0.45
grid.size <- 0.55

# Values passed to tmap.sdm - color scale info
blp1 <- tmap.sdm.help(ens.sf.unw, "Pred_ens")
blp2 <- tmap.sdm.help(ens.sf.unw, "CV")

# Plot of predictions (whales / km^-2)
tmap.obj1 <- tmap.sdm(
  ens.sf.unw, "Pred_ens", blp1, map.world, rpoly.mat, 
  "Unweighted ensemble - predictions", 
  main.size, leg.size, leg.width, grid.size
)
# Plot of SE values (with same color sceme as predictions)
tmap.obj2 <- tmap.sdm(
  ens.sf.unw, "SE", blp1, map.world, rpoly.mat, 
  "Unweighted ensemble - SE", 
  main.size, leg.size, leg.width, grid.size
)
# Plot of CV values
tmap.obj3 <- tmap.sdm(
  ens.sf.unw, "CV", blp2, map.world, rpoly.mat, 
  "Unweighted ensemble - CV", 
  main.size, leg.size, leg.width, grid.size
)

# Generate plot
tmap_arrange(
  list(tmap.obj1, tmap.obj2, tmap.obj3), ncol = 3, asp = NULL, outer.margins = 0.05
)
```

```{r, fig.height=9, fig.width=5.7, eval=FALSE}
### Figure 5

# Values passed to tmap.sdm - size of text labels and legend width
main.size <- 1.1
leg.size  <- 0.7
leg.width <- 0.6
grid.size <- 0.7

# Values passed to tmap.sdm - color scale info
blp1b <- tmap.sdm.help(ens.sf.wtss, "Pred_ens")
blp2b <- tmap.sdm.help.perc(ens.sf.wtss, "Pred_ens")

# Plot of predictions (whales / km^-2)
tmap.obj1 <- tmap.sdm(
  ens.sf.wtss, "Pred_ens", blp1, map.world, rpoly.mat, "Ensemble-TSS - Predictions", 
  main.size, leg.size, leg.width, grid.size
)
# Plot of SE values (with same color sceme as predictions)
tmap.obj2 <- tmap.sdm(
  ens.sf.wtss, "SE", blp1, map.world, rpoly.mat, "Ensemble-TSS - SE", 
  main.size, leg.size, leg.width, grid.size
)
# Plot of predictions (percentiles)
tmap.obj3 <- tmap.sdm(
  ens.sf.wtss, "Pred_ens", blp2b, map.world, rpoly.mat, "Ensemble-TSS - Predictions", 
  main.size, leg.size, leg.width, grid.size
)
# Plot of predictions (percentiles) with combined validation data presence points
tmap.obj4 <- tmap.sdm(
  ens.sf.wtss, "Pred_ens", blp2b, map.world, rpoly.mat, "Ensemble-TSS - Predictions", 
  main.size, leg.size, leg.width, grid.size, t.alpha = 0.8
) + 
  tm_shape(filter(valid.data, pres_abs == 1)) + 
  tm_dots(col = "black", size = 0.04, shape = 19)

# Generate plot
tmap_arrange(
  list(tmap.obj1, tmap.obj2, tmap.obj3, tmap.obj4), ncol = 2, nrow = 2, 
  asp = NULL, outer.margins = 0.05
)
```


## References

Becker, E., Forney, K., Fiedler, P., Barlow, J., Chivers, S., Edwards, C., ... Redfern, J. (2016) Moving towards dynamic ocean management: how well do modeled ocean products predict species distributions? Remote Sensing, 8, 149. 10.3390/rs8020149

Hazen, E.L., Palacios, D.M., Forney, K.A., Howell, E.A., Becker, E., Hoover, A.L., ... Bailey, H. (2017) WhaleWatch: a dynamic management tool for predicting blue whale density in the California Current. Journal of Applied Ecology, 54, 1415-1428. 10.1111/1365-2664.12820

Irvine, L.M., Mate, B.R., Winsor, M.H., Palacios, D.M., Bograd, S.J., Costa, D.P. & Bailey, H. (2014) Spatial and temporal occurrence of blue whales off the U.S. West Coast, with implications for management. PLoS One, 9, e102959. 10.1371/journal.pone.0102959

Redfern, J.V., Moore, T.J., Fiedler, P.C., de Vos, A., Brownell, R.L., Forney, K.A., ... Heikkinen, R. (2017) Predicting cetacean distributions in data-poor marine ecosystems. Diversity and Distributions, 23, 394-408. 10.1111/ddi.12537

Woodman, S.M., Forney, K.A., Becker, E.A., DeAngelis, M.L., Hazen, E.L., Palacios, D.M., Redfern, J.V. (in review) eSDM: A tool for creating and exploring ensembles of predictions from species distribution models.

